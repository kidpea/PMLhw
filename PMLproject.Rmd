---
title: "Weight Lifting Exercises Dataset analysis"
author: "Kidpea LAU"
date: "2018骞<b4>10<88>1<a5>"
output:
  html_document: default
  pdf_document: default
---
#Overview
The data is about Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: 
-exactly according to the specification (Class A)
-throwing the elbows to the front (Class B)
-lifting the dumbbell only halfway (Class C)
-lowering the dumbbell only halfway (Class D)
-throwing the hips to the front (Class E).

In the next , we will analysis the data and try to  find a predictor to classify the fashions. Then use the predictor to see if it s working well in the new data.

#DATA PREPROCESSING
###loading the TRAINING data into R
```{r echo=FALSE}
library(caret)
library(ggplot2)
library(rattle)
library(grid)
library(rattle)
library(rpart)
```
```{r}
if(!file.exists("./trainingdata.csv")) {
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
                destfile = "./trainingdata.csv")
}

trainData<-read.csv("./trainingdata.csv")
```
###CLEAN THE DATA :
```{r echo=FALSE}
train<- trainData[colSums(is.na(trainData))==0]
train<-train[,-c(1:7)]

nzv <- nearZeroVar(train[,-93])
if(length(nzv)>0) train <- train[,-nzv]

```
after cleaning the data, The data looks like  this :
```{r}
str(train)
```

set the data into training and valid to improve the preditor:
```{r}
inTrain <- createDataPartition(train$classe, p=0.7, list=FALSE)
training <- train[inTrain,]
pmlvalid <- train[-inTrain,]
```
let see the density about different ways of the  data:
```{r}
p1<-qplot(total_accel_belt,colour=classe,data=training,geom="density")
p2<-qplot(total_accel_arm,colour=classe,data=training,geom="density")
p3<-qplot(total_accel_dumbbell,colour=classe,data=training,geom="density")
p4<-qplot(total_accel_forearm,colour=classe,data=training,geom="density")
```
```{r echo=FALSE}
grid.newpage()  ###新建图表版面
pushViewport(viewport(layout = grid.layout(2,2))) ####将版面分成2*2矩阵
vplayout <- function(x,y){viewport(layout.pos.row = x, layout.pos.col = y)} 
print(p1, vp = vplayout(1,1))   ###将（1,1)和(1,2)的位置画图chart3
print(p2, vp = vplayout(1,2))     ###将(2,1)的位置画图chart2          
print(p3 , vp = vplayout(2,1))    ###将（2,2)的位置画图chart1
print(p4 , vp = vplayout(2,2)) 
```
#TREE Model
I will use trees to build the predictor,and check it out with fancy:
```{r}
set.seed(111)
treemodel <- rpart(classe ~ ., data=training, method="class")
fancyRpartPlot(treemodel, sub="")
```
Using confusion Matrix to test results:
```{r}
treePred<-predict(treemodel,pmlvalid,type = "class")
treeconf<-confusionMatrix(treePred,pmlvalid$classe)
treeAccuracy<-round(treeconf$overall['Accuracy'],4)
print(treeconf)

plot(treeconf$table,col=treeconf$byClass,main=paste(
       "DECISION TREE MODEL CONFUSION MATRIX:ACCURACY=",round(treeconf$overall['Accuracy'],4)))

```
As we can see, From the confusion matrix and the prediction accuracy (72.35%) of the combined model, there is no significant value in the added computational complexity for using the combined model for prediction.

#Random Forest
A random forest model was next applied to the dataset to see if it would lead to an improvement in prediction accuracy. For the Random Forest model, K-fold cross-validation is utilized.
```{r}
library(randomForest)
set.seed(222)
RFmodel <- randomForest(classe ~ ., data=training)
RFprediction <- predict(RFmodel, pmlvalid, type = "class")
RFconf <- confusionMatrix(RFprediction, pmlvalid$classe)

```
U can see the tree:
```{r echo=FALSE}
print(RFconf)
```
Using confusion Matrix to test results:
```{r}
RFaccuracy <- round(RFconf$overall['Accuracy'], 4)
plot(RFconf$table, col = RFconf$byClass, main = paste(
       "Random Forest model confusion matrix: Accuracy =", round(RFconf$overall['Accuracy'], 4)))
```
The random forest model had an overall prediction accuracy of 99.42%. This accuracy was much higher than that found for the simple tree model. For the model calculated using the Random Forest method, the out of sample error rate is 0.58%.

#predict the new testing data:
For Random Forests we use the following formula, which yielded a much better prediction in in-sample:
```{r}
#test
if(!file.exists("./testingdata.csv")) {
  download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
                destfile = "./testingdata.csv")
}

testData<-read.csv("./testingdata.csv")
library(randomForest)
RFpredictSubmit <- predict(RFmodel, testData, type = "class")
results <- data.frame("Participant"=testData$user_name, "Problem_id"=testData$problem_id, 
                      "Class"=RFpredictSubmit)
print(results)
```

